{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import yaml\n",
    "import joblib\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "from torchview import draw_graph\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomException(Exception):\n",
    "    def __init__(self, message: str):\n",
    "        super(CustomException, self).__init__()\n",
    "        self.message = message\n",
    "\n",
    "\n",
    "def dump(value: str, filename: str):\n",
    "    if (value is not None) and (filename is not None):\n",
    "        joblib.dump(value=value, filename=filename)\n",
    "\n",
    "    else:\n",
    "        raise CustomException(\"Cannot be dump into pickle file\".capitalize())\n",
    "\n",
    "\n",
    "def load(filename: str):\n",
    "    if filename is not None:\n",
    "        joblib.load(filename=filename)\n",
    "\n",
    "    else:\n",
    "        raise CustomException(\"Cannot be load the pickle file\".capitalize())\n",
    "\n",
    "\n",
    "def device_init(self, device: str = \"mps\"):\n",
    "    if device == \"cuda\":\n",
    "        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    elif device == \"mps\":\n",
    "        return torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def config():\n",
    "    with open(\"./config.yml\", \"r\") as file:\n",
    "        return yaml.safe_load(file)\n",
    "\n",
    "\n",
    "english = [\n",
    "    \"The sun is shining brightly today\",\n",
    "    \"I enjoy reading books on rainy afternoons\",\n",
    "    \"The cat sat on the windowsill watching the birds\",\n",
    "    \"She baked a delicious chocolate cake for dessert\",\n",
    "    \"We went for a long walk in the park yesterday\",\n",
    "    \"He plays the guitar beautifully during the evenings\",\n",
    "]\n",
    "\n",
    "german = [\n",
    "    \"Die Sonne scheint heute hell\",\n",
    "    \"Ich lese gerne Bücher an regnerischen Nachmittagen\",\n",
    "    \"Die Katze saß auf der Fensterbank und beobachtete die Vögel\",\n",
    "    \"Sie hat einen leckeren Schokoladenkuchen zum Nachtisch gebacken\",\n",
    "    \"Wir sind gestern lange im Park spazieren gegangen\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(\n",
    "        self, sequence_length: int = 200, dimension: int = 512, constant: int = 10000\n",
    "    ):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        self.sequence_length = sequence_length\n",
    "        self.model_dimension = dimension\n",
    "        self.constant = constant\n",
    "\n",
    "        self.position_encode = torch.ones((sequence_length, dimension))\n",
    "\n",
    "        for position in tqdm(range(self.sequence_length)):\n",
    "            for index in range(self.model_dimension):\n",
    "                if index % 2 == 0:\n",
    "                    self.position_encode[position, index] = math.sin(\n",
    "                        position / self.constant ** (2 * index / self.model_dimension)\n",
    "                    )\n",
    "                elif index % 2 != 0:\n",
    "                    self.position_encode[position, index] = math.cos(\n",
    "                        position / self.constant ** (2 * index / self.model_dimension)\n",
    "                    )\n",
    "\n",
    "        self.register_buffer(\"position_encoding\", self.position_encode.unsqueeze(0))\n",
    "\n",
    "        print(\"Positional Encoding initialized\".capitalize())\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            return self.position_encode[:, : x.shape[-1]]\n",
    "\n",
    "        else:\n",
    "            raise TypeError(\"Input must be a torch.Tensor\".capitalize())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Positional Encoder for Transformer\".title()\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--seq_length\",\n",
    "        type=int,\n",
    "        default=200,\n",
    "        help=\"Define the sequence length\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dimension\",\n",
    "        type=int,\n",
    "        default=512,\n",
    "        help=\"Define the dimension of the model\".capitalize(),\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    sequence_length = args.seq_length\n",
    "    model_dimension = args.dimension\n",
    "\n",
    "    positional_encode = PositionalEncoding(\n",
    "        sequence_length=sequence_length, dimension=model_dimension\n",
    "    )\n",
    "\n",
    "    assert positional_encode(\n",
    "        torch.randn((sequence_length, model_dimension))\n",
    "    ).size() == (\n",
    "        sequence_length,\n",
    "        model_dimension,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocabulary_size: int = 1000,\n",
    "        sequence_length: int = 200,\n",
    "        dimension: int = 100,\n",
    "    ):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.model_dimension = dimension\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=self.vocabulary_size, embedding_dim=self.model_dimension\n",
    "        )\n",
    "\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            sequence_length=self.sequence_length, dimension=self.model_dimension\n",
    "        )\n",
    "\n",
    "    def forward(self, tokenize: torch.Tensor):\n",
    "        if isinstance(tokenize, torch.Tensor):\n",
    "            x = self.embedding(tokenize)\n",
    "            return x + self.positional_encoding(x)\n",
    "\n",
    "        else:\n",
    "            raise TypeError(\"Input must be a torch.Tensor\".capitalize())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Embedding Layer for Transformer\".title()\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--vocab_size\", type=int, default=100, help=\"Vocabulary Size\".capitalize()\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--seq_len\", type=int, default=200, help=\"Sequence Length\".capitalize()\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dim\", type=int, default=512, help=\"Dimension of the Model\".capitalize()\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    sequence_length = args.seq_len\n",
    "    vocabulary_size = args.vocab_size\n",
    "    model_dimension = args.dim\n",
    "\n",
    "    embedding = EmbeddingLayer(\n",
    "        vocabulary_size=vocabulary_size,\n",
    "        sequence_length=sequence_length,\n",
    "        dimension=model_dimension,\n",
    "    )\n",
    "    input_ids = torch.randint(0, vocabulary_size, (400, sequence_length))\n",
    "\n",
    "    assert embedding(input_ids).size() == (\n",
    "        400,\n",
    "        sequence_length,\n",
    "        model_dimension,\n",
    "    ), \"Dimension Mismatch in the embedding layer\".title()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, normalized_shape: int = 512, epsilon: float = 1e-5):\n",
    "        super(LayerNormalization, self).__init__()\n",
    "\n",
    "        self.normalized_shape = normalized_shape\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.gamma = nn.Parameter(\n",
    "            data=torch.ones((normalized_shape,)), requires_grad=True\n",
    "        )\n",
    "        self.beta = nn.Parameter(\n",
    "            data=torch.zeros((normalized_shape,)), requires_grad=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            self.mean = torch.mean(x, dim=-1)\n",
    "            self.variance = torch.var(x, dim=-1)\n",
    "\n",
    "            self.mean = self.mean.unsqueeze(-1)\n",
    "            self.variance = self.variance.unsqueeze(-1)\n",
    "\n",
    "            return (x - self.mean) / torch.sqrt(\n",
    "                self.variance + self.epsilon\n",
    "            ) * self.gamma + self.beta\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Layer Normalization for transformer\".title()\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--normalized_shape\",\n",
    "        type=int,\n",
    "        default=512,\n",
    "        help=\"The normalized shape of the input tensor\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--epsilon\",\n",
    "        type=float,\n",
    "        default=1e-6,\n",
    "        help=\"The epsilon value for the variance\".capitalize(),\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    normalized_shape = args.normalized_shape\n",
    "    epsilon = args.epsilon\n",
    "\n",
    "    layer_norm = LayerNormalization(normalized_shape=normalized_shape, epsilon=epsilon)\n",
    "\n",
    "    assert layer_norm(torch.randn((40, 200, 512))).size() == (\n",
    "        40,\n",
    "        200,\n",
    "        normalized_shape,\n",
    "    ), \"Layer Normalization is not working properly, check the dimensions\".title()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PointWise Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointWiseFeedForward(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int = 512,\n",
    "        out_features: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "        display: bool = False,\n",
    "    ):\n",
    "        super(PointWiseFeedForward, self).__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.dropout = dropout\n",
    "        self.display = display\n",
    "\n",
    "        self.layers = list()\n",
    "\n",
    "        for index in range(2):\n",
    "            self.layers.append(\n",
    "                nn.Linear(\n",
    "                    in_features=self.in_features,\n",
    "                    out_features=self.out_features,\n",
    "                    bias=False,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            self.in_features = self.out_features\n",
    "            self.out_features = in_features\n",
    "\n",
    "            if index == 0:\n",
    "                self.layers.append(nn.ReLU(inplace=True))\n",
    "                self.layers.append(nn.Dropout1d(p=self.dropout))\n",
    "\n",
    "        self.model = nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            return self.model(x)\n",
    "\n",
    "        else:\n",
    "            raise TypeError(\"Input type is not a torch.Tensor\".capitalize())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Feed Forward Network for Transformer\".title()\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--in_features\", type=int, default=512, help=\"Input features\".capitalize()\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--out_features\", type=int, default=2048, help=\"Output features\".capitalize()\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dropout\",\n",
    "        type=float,\n",
    "        default=0.1,\n",
    "        help=\"Dropout rate\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--display\",\n",
    "        type=bool,\n",
    "        default=False,\n",
    "        help=\"Display the model\".capitalize(),\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    net = PointWiseFeedForward(\n",
    "        in_features=args.in_features, out_features=args.out_features\n",
    "    )\n",
    "\n",
    "    assert net(torch.randn((40, 200, args.in_features))).size() == (\n",
    "        40,\n",
    "        200,\n",
    "        args.in_features,\n",
    "    )\n",
    "    if args.display:\n",
    "        print(summary(model=net, input_size=(200, 512)))\n",
    "\n",
    "        path = config()[\"path\"][\"FILES_PATH\"]\n",
    "\n",
    "        draw_graph(\n",
    "            model=net, input_data=torch.randn((40, 200, 512))\n",
    "        ).visual_graph.render(\n",
    "            filename=os.path.join(path, \"feedforward_network\"), format=\"png\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_mask(mask: torch.Tensor):\n",
    "    mask = torch.where(mask == 0.0, float(\"-inf\"), mask)\n",
    "    return mask.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "\n",
    "def target_mask(sequence_length: int = 200):\n",
    "    mask = torch.triu(input=torch.ones((sequence_length, sequence_length)), diagonal=1)\n",
    "    mask = torch.where(mask == 1.0, float(\"-inf\"), mask)\n",
    "    return mask\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mask = padding_mask(mask=torch.ones((40, 200)))\n",
    "    assert mask.size() == (40, 1, 1, 200)\n",
    "\n",
    "    mask = target_mask(sequence_length=200)\n",
    "    print(mask.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Attention - Scaled dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(\n",
    "    query: torch.Tensor,\n",
    "    key: torch.Tensor,\n",
    "    values: torch.Tensor,\n",
    "    mask=None,\n",
    "    type: str = \"src\",\n",
    "):\n",
    "    if (\n",
    "        isinstance(query, torch.Tensor)\n",
    "        and isinstance(key, torch.Tensor)\n",
    "        and isinstance(values, torch.Tensor)\n",
    "    ) and (query.size() == key.size() == values.size()):\n",
    "\n",
    "        result = torch.matmul(query, key.transpose(-1, -2)) / math.sqrt(\n",
    "            (query.size(1) * query.size(3))\n",
    "        )\n",
    "\n",
    "        if (mask is not None) and (type == \"src\"):\n",
    "            result += padding_mask(mask=mask)\n",
    "\n",
    "        elif type == \"target\":\n",
    "            result += (\n",
    "                target_mask(sequence_length=result.size(-1)).unsqueeze(0).unsqueeze(1)\n",
    "            )\n",
    "\n",
    "        result = torch.softmax(input=result, dim=-1)\n",
    "\n",
    "        result = torch.matmul(result, values)\n",
    "\n",
    "        return result\n",
    "    else:\n",
    "        raise TypeError(\n",
    "            \"The query, key, and values must be of type torch.Tensor and same shape\".capitalize()\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Scaled dot product for Transformer\".title()\n",
    "    )\n",
    "    parser.parse_args()\n",
    "\n",
    "    query = torch.randn((40, 8, 200, 64))\n",
    "    key = torch.randn((40, 8, 200, 64))\n",
    "    values = torch.randn((40, 8, 200, 64))\n",
    "\n",
    "    attention_output = scaled_dot_product(query=query, key=key, values=values)\n",
    "\n",
    "    assert attention_output.size() == (40, 8, 200, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multihead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, dimension=512, heads: int = 8, dropout: float = 0.1):\n",
    "        super(MultiHeadAttentionLayer, self).__init__()\n",
    "\n",
    "        self.dimension = dimension\n",
    "        self.heads = heads\n",
    "        self.dropout = dropout\n",
    "\n",
    "        assert (\n",
    "            self.dimension % self.heads == 0\n",
    "        ), \"Dimension must be divisible by heads\".title()\n",
    "\n",
    "        self.QKV = nn.Linear(\n",
    "            in_features=self.dimension, out_features=3 * self.dimension, bias=False\n",
    "        )\n",
    "        self.layer = nn.Linear(\n",
    "            in_features=self.dimension, out_features=self.dimension, bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask=None):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            self.mask = mask\n",
    "\n",
    "            QKV = self.QKV(x)\n",
    "\n",
    "            self.query, self.key, self.values = torch.chunk(input=QKV, chunks=3, dim=-1)\n",
    "\n",
    "            self.query = self.query.view(\n",
    "                self.query.size(0),\n",
    "                self.query.size(1),\n",
    "                self.heads,\n",
    "                self.dimension // self.heads,\n",
    "            )\n",
    "            self.key = self.key.view(\n",
    "                self.key.size(0),\n",
    "                self.key.size(1),\n",
    "                self.heads,\n",
    "                self.dimension // self.heads,\n",
    "            )\n",
    "            self.values = self.values.view(\n",
    "                self.values.size(0),\n",
    "                self.values.size(1),\n",
    "                self.heads,\n",
    "                self.dimension // self.heads,\n",
    "            )\n",
    "\n",
    "            self.query = self.query.permute(0, 2, 1, 3)\n",
    "            self.key = self.key.permute(0, 2, 1, 3)\n",
    "            self.values = self.values.permute(0, 2, 1, 3)\n",
    "\n",
    "            try:\n",
    "                self.attention = scaled_dot_product(\n",
    "                    query=self.query, key=self.key, values=self.values, mask=self.mask\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"An error occured : {}\".format(e))\n",
    "\n",
    "            else:\n",
    "                self.attention = self.attention.view(\n",
    "                    self.attention.size(0),\n",
    "                    self.attention.size(2),\n",
    "                    self.attention.size(1) * self.attention.size(3),\n",
    "                )\n",
    "\n",
    "                return self.layer(self.attention)\n",
    "\n",
    "        else:\n",
    "            raise TypeError(\"Input must be a torch.Tensor\".capitalize())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"MultiHeadAttention Layer for Transformer\".title()\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dimension\",\n",
    "        type=int,\n",
    "        default=512,\n",
    "        help=\"Dimension of the input tensor\".title(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--heads\",\n",
    "        type=int,\n",
    "        default=8,\n",
    "        help=\"Number of heads for the multihead attention\".title(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dropout\",\n",
    "        type=float,\n",
    "        default=0.1,\n",
    "        help=\"Dropout rate for the multihead attention\".title(),\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    dimension = args.dimension\n",
    "    heads = args.heads\n",
    "    dropout = args.dropout\n",
    "\n",
    "    attention = MultiHeadAttentionLayer(\n",
    "        dimension=dimension, heads=heads, dropout=dropout\n",
    "    )\n",
    "\n",
    "    input = torch.randn((40, 200, dimension))\n",
    "    masked = torch.ones((40, 200))\n",
    "\n",
    "    assert attention(input, masked).size() == (\n",
    "        40,\n",
    "        200,\n",
    "        dimension,\n",
    "    ), \"Dimension of the output tensor must be equal to the input dimension\".capitalize()\n",
    "\n",
    "    masked = None\n",
    "\n",
    "    assert attention(input).size() == (\n",
    "        40,\n",
    "        200,\n",
    "        dimension,\n",
    "    ), \"Dimension of the output tensor must be equal to the input dimension\".capitalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dimension: int = 512,\n",
    "        heads: int = 8,\n",
    "        feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "        epsilon: float = 1e-6,\n",
    "        display: bool = False,\n",
    "    ):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "\n",
    "        self.dimension = dimension\n",
    "        self.heads = heads\n",
    "        self.feedforward = feedforward\n",
    "        self.dropout = dropout\n",
    "        self.epsilon = epsilon\n",
    "        self.display = display\n",
    "\n",
    "        self.multihead_attention = MultiHeadAttentionLayer(\n",
    "            dimension=self.dimension, heads=self.heads, dropout=self.dropout\n",
    "        )\n",
    "\n",
    "        self.layer_norm = LayerNormalization(\n",
    "            normalized_shape=self.dimension, epsilon=self.epsilon\n",
    "        )\n",
    "\n",
    "        self.feedforward_network = PointWiseFeedForward(\n",
    "            in_features=self.dimension,\n",
    "            out_features=self.feedforward,\n",
    "            dropout=self.dropout,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask=None):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            self.mask = mask\n",
    "\n",
    "            residual = x\n",
    "\n",
    "            x = self.multihead_attention(x=x, mask=self.mask)\n",
    "            x = torch.dropout(input=x, p=self.dropout, train=self.training)\n",
    "            x = torch.add(x, residual)\n",
    "            x = self.layer_norm(x)\n",
    "\n",
    "            residual = x\n",
    "\n",
    "            x = self.feedforward_network(x=x)\n",
    "            x = torch.dropout(input=x, p=self.dropout, train=self.training)\n",
    "            x = torch.add(residual, x)\n",
    "            x = self.layer_norm(x)\n",
    "\n",
    "            return x\n",
    "\n",
    "        else:\n",
    "            raise TypeError(\"Input must be a tensor\".capitalize())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Encoder Block for Transfomers\".title()\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--dimension\",\n",
    "        type=int,\n",
    "        default=512,\n",
    "        help=\"Dimension of the input tensor\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--heads\",\n",
    "        type=int,\n",
    "        default=8,\n",
    "        help=\"Number of heads in the multi-head attention\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--feedfoward\",\n",
    "        type=int,\n",
    "        default=2048,\n",
    "        help=\"Dimension of the feedforward network\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dropout\", type=float, default=0.1, help=\"Dropout rate\".capitalize()\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-eps\",\n",
    "        type=float,\n",
    "        default=1e-6,\n",
    "        help=\"Epsilon value for layer normalization\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--display\", type=bool, default=False, help=\"Display the model\".capitalize()\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    dimension = args.dimension\n",
    "    heads = args.heads\n",
    "    dropout = args.dropout\n",
    "    feedforward = args.feedfoward\n",
    "    eps = args.eps\n",
    "    display = args.display\n",
    "\n",
    "    encoder = EncoderBlock(\n",
    "        dimension=dimension,\n",
    "        heads=heads,\n",
    "        dropout=dropout,\n",
    "    )\n",
    "    masked = torch.ones((40, 200))\n",
    "\n",
    "    assert encoder(torch.randn((40, 200, dimension)), masked).size() == (\n",
    "        40,\n",
    "        200,\n",
    "        dimension,\n",
    "    ), \"Encoder block is not working properl as dimension is not equal\".title()\n",
    "\n",
    "    masked = None\n",
    "\n",
    "    assert encoder(torch.randn((40, 200, dimension))).size() == (\n",
    "        40,\n",
    "        200,\n",
    "        dimension,\n",
    "    ), \"Encoder block is not working properl as dimension is not equal\".title()\n",
    "\n",
    "    if display:\n",
    "\n",
    "        path = config()[\"path\"][\"FILES_PATH\"]\n",
    "\n",
    "        draw_graph(\n",
    "            model=encoder, input_data=torch.randn((40, 200, dimension))\n",
    "        ).visual_graph.render(filename=os.path.join(path, \"one_encoder\"), format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int = 512,\n",
    "        nhead: int = 8,\n",
    "        dim_feedforward: int = 2048,\n",
    "        num_encoder_layers: int = 8,\n",
    "        dropout: float = 0.1,\n",
    "        layer_norm_eps: float = 1e-6,\n",
    "        display: bool = False,\n",
    "    ):\n",
    "\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "\n",
    "        self.dimension = d_model\n",
    "        self.heads = nhead\n",
    "        self.feedforward = dim_feedforward\n",
    "        self.num_encoder_layers = num_encoder_layers\n",
    "        self.dropout = dropout\n",
    "        self.epsilon = layer_norm_eps\n",
    "        self.display = display\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            *[\n",
    "                EncoderBlock(\n",
    "                    dimension=self.dimension,\n",
    "                    heads=self.heads,\n",
    "                    feedforward=self.feedforward,\n",
    "                    dropout=self.dropout,\n",
    "                    epsilon=self.epsilon,\n",
    "                )\n",
    "                for _ in tqdm(range(self.num_encoder_layers))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask=None):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            for layer in self.encoder:\n",
    "                x = layer(x, mask)\n",
    "            return x\n",
    "\n",
    "        else:\n",
    "            raise TypeError(\"Input must be a tensor\".capitalize())\n",
    "\n",
    "    @staticmethod\n",
    "    def display_parameters(model: nn.Module):\n",
    "        if isinstance(model, TransformerEncoder):\n",
    "            return sum(params.numel() for params in model.parameters())\n",
    "\n",
    "        else:\n",
    "            raise TypeError(\"Input must be a transformer encoder\".capitalize())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Encoder for Transformer\".title())\n",
    "    parser.add_argument(\n",
    "        \"--d_model\", type=int, default=512, help=\"Dimension of the model\".capitalize()\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--nhead\", type=int, default=8, help=\"Number of heads\".capitalize()\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--feedforward\",\n",
    "        type=int,\n",
    "        default=2048,\n",
    "        help=\"Feedforward dimension\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dropout\", type=float, default=0.1, help=\"Dropout rate\".capitalize()\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--epsilon\", type=float, default=1e-6, help=\"Epsilon for LayerNorm\".capitalize()\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--display\", type=bool, default=False, help=\"Display the model\".capitalize()\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_encoder_layers\",\n",
    "        type=int,\n",
    "        default=8,\n",
    "        help=\"Number of encoder layers\".capitalize(),\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    d_model = args.d_model\n",
    "    nheads = args.nhead\n",
    "    feedforward = args.feedforward\n",
    "    num_encoder_layers = args.num_encoder_layers\n",
    "    dropout = args.dropout\n",
    "    epsilon = args.epsilon\n",
    "    display = args.display\n",
    "\n",
    "    input = torch.randn((40, 200, d_model))\n",
    "    masked = torch.ones((40, 200))\n",
    "\n",
    "    encoderTransformer = TransformerEncoder(\n",
    "        d_model=d_model,\n",
    "        nhead=nheads,\n",
    "        dim_feedforward=feedforward,\n",
    "        dropout=dropout,\n",
    "        layer_norm_eps=epsilon,\n",
    "        display=display,\n",
    "        num_encoder_layers=num_encoder_layers,\n",
    "    )\n",
    "\n",
    "    assert encoderTransformer(input, masked).size() == (\n",
    "        40,\n",
    "        200,\n",
    "        d_model,\n",
    "    ), \"Transformer Encoder block is not working properl as dimension is not equal\"\n",
    "\n",
    "    masked = None\n",
    "\n",
    "    assert encoderTransformer(input).size() == (\n",
    "        40,\n",
    "        200,\n",
    "        d_model,\n",
    "    ), \"Transformer Encoder block is not working properl as dimension is not equal\"\n",
    "\n",
    "    if display:\n",
    "        print(\n",
    "            f\"Total parameters of the transformer encoder {TransformerEncoder.display_parameters(model=encoderTransformer)}\"\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            path = config()[\"path\"][\"FILES_PATH\"]\n",
    "\n",
    "            draw_graph(\n",
    "                model=encoderTransformer, input_data=torch.randn((40, 200, d_model))\n",
    "            ).visual_graph.render(\n",
    "                filename=os.path.join(path, \"encoderTransformer\"), format=\"png\"\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"An error occurred: \", e)\n",
    "\n",
    "        else:\n",
    "            print(f\"Encoder Transformer graph saved successfully in the path {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiCross Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiCrossAttentionLayer(nn.Module):\n",
    "    def __init__(self, dimension: int = 512, heads: int = 8, dropout: float = 0.1):\n",
    "        super(MultiCrossAttentionLayer, self).__init__()\n",
    "\n",
    "        self.dimension = dimension\n",
    "        self.heads = heads\n",
    "        self.dropout = dropout\n",
    "\n",
    "        assert (\n",
    "            self.dimension % self.heads == 0\n",
    "        ), \"Dimension must be divisible by heads\".title()\n",
    "\n",
    "        self.KV = nn.Linear(\n",
    "            in_features=self.dimension, out_features=2 * self.dimension, bias=False\n",
    "        )\n",
    "        self.Q = nn.Linear(\n",
    "            in_features=self.dimension, out_features=self.dimension, bias=False\n",
    "        )\n",
    "\n",
    "        self.layer = nn.Linear(\n",
    "            in_features=self.dimension, out_features=self.dimension, bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, y: torch.Tensor, mask=None):\n",
    "        if isinstance(x, torch.Tensor) and isinstance(y, torch.Tensor):\n",
    "            self.mask = mask\n",
    "\n",
    "            KV = self.KV(x)\n",
    "            Q = self.Q(y)\n",
    "\n",
    "            self.key, self.value = torch.chunk(input=KV, chunks=2, dim=-1)\n",
    "            self.query = Q\n",
    "\n",
    "            self.key = self.key.view(\n",
    "                self.key.size(0),\n",
    "                self.key.size(1),\n",
    "                self.heads,\n",
    "                self.dimension // self.heads,\n",
    "            )\n",
    "            self.value = self.value.view(\n",
    "                self.value.size(0),\n",
    "                self.value.size(1),\n",
    "                self.heads,\n",
    "                self.dimension // self.heads,\n",
    "            )\n",
    "            self.query = self.query.view(\n",
    "                self.query.size(0),\n",
    "                self.query.size(1),\n",
    "                self.heads,\n",
    "                self.dimension // self.heads,\n",
    "            )\n",
    "\n",
    "            self.key = self.key.permute(0, 2, 1, 3)\n",
    "            self.value = self.value.permute(0, 2, 1, 3)\n",
    "            self.query = self.query.permute(0, 2, 1, 3)\n",
    "\n",
    "            result = scaled_dot_product(\n",
    "                query=self.query,\n",
    "                key=self.key,\n",
    "                values=self.value,\n",
    "                type=\"target\",\n",
    "            )\n",
    "            result = result.view(\n",
    "                result.size(0), result.size(2), result.size(1) * result.size(3)\n",
    "            )\n",
    "            return self.layer(result)\n",
    "\n",
    "        else:\n",
    "            raise TypeError(\"x and y must be torch.Tensor\".capitalize())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Multi Cross Attention Layer for Transformer\".title()\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dimension\", type=int, default=512, help=\"dimension\".capitalize()\n",
    "    )\n",
    "    parser.add_argument(\"--heads\", type=int, default=8, help=\"heads\".capitalize())\n",
    "    parser.add_argument(\n",
    "        \"--dropout\", type=float, default=0.1, help=\"dropout\".capitalize()\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    dimension = args.dimension\n",
    "    heads = args.heads\n",
    "    dropout = args.dropout\n",
    "\n",
    "    attention = MultiCrossAttentionLayer(\n",
    "        dimension=dimension,\n",
    "        heads=heads,\n",
    "        dropout=dropout,\n",
    "    )\n",
    "\n",
    "    assert attention(\n",
    "        torch.randn((40, 200, dimension)), torch.randn((40, 200, dimension))\n",
    "    ).size() == (\n",
    "        40,\n",
    "        200,\n",
    "        dimension,\n",
    "    ), \"Multi Cross Attention Layer for Transformer is not working properly\".capitalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dimension: int = 512,\n",
    "        heads: int = 8,\n",
    "        feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "        epsilon: float = 1e-6,\n",
    "        display: bool = False,\n",
    "    ):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        self.dimension = dimension\n",
    "        self.heads = heads\n",
    "        self.feedforward = feedforward\n",
    "        self.dropout = dropout\n",
    "        self.epsilon = epsilon\n",
    "        self.display = display\n",
    "\n",
    "        self.masked_multihead_attention = MultiHeadAttentionLayer(\n",
    "            dimension=self.dimension,\n",
    "            heads=self.heads,\n",
    "            dropout=self.dropout,\n",
    "        )\n",
    "\n",
    "        self.layer_norm = LayerNormalization(\n",
    "            normalized_shape=self.dimension,\n",
    "            epsilon=self.epsilon,\n",
    "        )\n",
    "\n",
    "        self.encoder_deecoder_attention = MultiCrossAttentionLayer(\n",
    "            dimension=self.dimension,\n",
    "            heads=self.heads,\n",
    "            dropout=self.dropout,\n",
    "        )\n",
    "\n",
    "        self.feedforward_network = PointWiseFeedForward(\n",
    "            in_features=self.dimension,\n",
    "            dropout=self.dropout,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, y: torch.Tensor, mask=None):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            residual = y\n",
    "\n",
    "            y = self.masked_multihead_attention(x=y, mask=mask)\n",
    "            y = torch.dropout(input=y, p=self.dropout, train=self.training)\n",
    "            y = torch.add(y, residual)\n",
    "            y = self.layer_norm(y)\n",
    "\n",
    "            residual = y\n",
    "\n",
    "            y = self.encoder_deecoder_attention(x=x, y=y, mask=None)\n",
    "            y = torch.dropout(input=y, p=self.dropout, train=self.training)\n",
    "            y = torch.add(y, residual)\n",
    "            y = self.layer_norm(y)\n",
    "\n",
    "            residual = y\n",
    "\n",
    "            y = self.feedforward_network(y)\n",
    "            y = torch.dropout(input=y, p=self.dropout, train=self.training)\n",
    "            y = torch.add(y, residual)\n",
    "            y = self.layer_norm(y)\n",
    "\n",
    "            return y\n",
    "        else:\n",
    "            raise TypeError(\"Input must be a torch.Tensor\".capitalize())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Decoder block for the Transformer model\".title()\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dimension\",\n",
    "        type=int,\n",
    "        default=512,\n",
    "        help=\"Dimension of the input tensor\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--heads\",\n",
    "        type=int,\n",
    "        default=8,\n",
    "        help=\"Number of attention heads\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dropout\",\n",
    "        type=float,\n",
    "        default=0.1,\n",
    "        help=\"Dropout rate\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--epsilon\",\n",
    "        type=float,\n",
    "        default=1e-6,\n",
    "        help=\"Epsilon for the layer normalization\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--feedforward\",\n",
    "        type=int,\n",
    "        default=2048,\n",
    "        help=\"Feedforward dimension\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--display\",\n",
    "        type=bool,\n",
    "        default=False,\n",
    "        help=\"Display the model architecture\".capitalize(),\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    dimension = args.dimension\n",
    "    heads = args.heads\n",
    "    dropout = args.dropout\n",
    "    epsilon = args.epsilon\n",
    "    feedforward = args.feedforward\n",
    "\n",
    "    decoder = DecoderBlock(\n",
    "        dimension=dimension,\n",
    "        heads=heads,\n",
    "        dropout=dropout,\n",
    "        epsilon=epsilon,\n",
    "        feedforward=feedforward,\n",
    "    )\n",
    "    X = torch.randn((40, 200, dimension))\n",
    "    y = torch.randn((40, 200, dimension))\n",
    "\n",
    "    assert decoder(X, y).size() == (\n",
    "        40,\n",
    "        200,\n",
    "        dimension,\n",
    "    ), \"Model output size is incorrect from decoder block\".capitalize()\n",
    "\n",
    "    if args.display:\n",
    "        path = config()[\"path\"][\"FILES_PATH\"]\n",
    "\n",
    "        draw_graph(model=decoder, input_data=(X, y)).visual_graph.render(\n",
    "            filename=os.path.join(path, \"one_decoder\"), format=\"png\"\n",
    "        )\n",
    "\n",
    "        print(f\"Model architecture is saved in the path {path}\".capitalize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int = 512,\n",
    "        nhead: int = 8,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "        layer_norm_eps: float = 1e-6,\n",
    "        num_decoder_layers: int = 6,\n",
    "        display: bool = False,\n",
    "    ):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "\n",
    "        self.dimension = d_model\n",
    "        self.heads = nhead\n",
    "        self.feedforward = dim_feedforward\n",
    "        self.dropout = dropout\n",
    "        self.epsilon = layer_norm_eps\n",
    "        self.number_of_layers = num_decoder_layers\n",
    "        self.display = display\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            *[\n",
    "                DecoderBlock(\n",
    "                    dimension=self.dimension,\n",
    "                    heads=self.heads,\n",
    "                    feedforward=self.feedforward,\n",
    "                    dropout=self.dropout,\n",
    "                    epsilon=self.epsilon,\n",
    "                )\n",
    "                for _ in tqdm(range(self.number_of_layers))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, y: torch.Tensor, mask=None):\n",
    "        if isinstance(x, torch.Tensor) and (isinstance(y, torch.Tensor)):\n",
    "            for layer in self.decoder:\n",
    "                y = layer(x=x, y=y, mask=mask)\n",
    "\n",
    "            return y\n",
    "\n",
    "        else:\n",
    "            raise TypeError(\n",
    "                \"Input and output must be of type torch.Tensor\".capitalize()\n",
    "            )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Decoder layer for the transformer\".title()\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--d_model\", type=int, default=512, help=\"Dimension of the model\".capitalize()\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_decoder_layers\",\n",
    "        type=int,\n",
    "        default=8,\n",
    "        help=\"Number of decoder layers\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dim_feedforward\",\n",
    "        type=int,\n",
    "        default=2048,\n",
    "        help=\"Dimension of the feedforward layer\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dropout\", type=float, default=0.1, help=\"Dropout rate\".capitalize()\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--layer_norm_eps\",\n",
    "        type=float,\n",
    "        default=1e-6,\n",
    "        help=\"Layer norm epsilon\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--display\", type=bool, default=False, help=\"Display the model\".capitalize()\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    d_model = args.d_model\n",
    "    num_decoder_layers = args.num_decoder_layers\n",
    "    dim_feedforward = args.dim_feedforward\n",
    "    dropout = args.dropout\n",
    "    layer_norm_eps = args.layer_norm_eps\n",
    "\n",
    "    decoderTransformer = TransformerDecoder(\n",
    "        d_model=d_model,\n",
    "        num_decoder_layers=num_decoder_layers,\n",
    "        dim_feedforward=dim_feedforward,\n",
    "        dropout=dropout,\n",
    "        layer_norm_eps=layer_norm_eps,\n",
    "    )\n",
    "\n",
    "    X = torch.randn((40, 200, d_model))\n",
    "    y = torch.randn((40, 200, d_model))\n",
    "    padding_masked = torch.randn((40, 200))\n",
    "\n",
    "    assert decoderTransformer(X, y, padding_masked).size() == (\n",
    "        40,\n",
    "        200,\n",
    "        d_model,\n",
    "    ), \"Dimension mismatch in the decoder\".title()\n",
    "\n",
    "    if args.display:\n",
    "        print(\n",
    "            \"Total parameters of the decoder transformer: \",\n",
    "            sum(params.numel() for params in decoderTransformer.parameters()),\n",
    "        )\n",
    "        path = config()[\"path\"][\"FILES_PATH\"]\n",
    "\n",
    "        draw_graph(\n",
    "            model=decoderTransformer, input_data=(X, y, padding_masked)\n",
    "        ).visual_graph.render(\n",
    "            filename=os.path.join(path, \"decoderTransformer\"), format=\"png\"\n",
    "        )\n",
    "\n",
    "        print(f\"Decoder model saved in the path {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int = 512,\n",
    "        nhead: int = 8,\n",
    "        num_encoder_layers: int = 8,\n",
    "        num_decoder_layers: int = 6,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "        layer_norm_eps: float = 1e-05,\n",
    "    ):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.num_encoder_layers = num_encoder_layers\n",
    "        self.num_decoder_layers = num_decoder_layers\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.dropout = dropout\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "\n",
    "        self.transformerEncoder = TransformerEncoder(\n",
    "            d_model=self.d_model,\n",
    "            nhead=self.nhead,\n",
    "            dim_feedforward=self.dim_feedforward,\n",
    "            num_encoder_layers=self.num_encoder_layers,\n",
    "            dropout=self.dropout,\n",
    "            layer_norm_eps=self.layer_norm_eps,\n",
    "        )\n",
    "\n",
    "        self.transformerDecoder = TransformerDecoder(\n",
    "            d_model=self.d_model,\n",
    "            nhead=self.nhead,\n",
    "            dim_feedforward=self.dim_feedforward,\n",
    "            num_decoder_layers=self.num_decoder_layers,\n",
    "            dropout=self.dropout,\n",
    "            layer_norm_eps=self.layer_norm_eps,\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        y: torch.Tensor,\n",
    "        encoder_padding_mask=None,\n",
    "        decoder_padding_mask=None,\n",
    "    ):\n",
    "        if isinstance(x, torch.Tensor) and isinstance(y, torch.Tensor):\n",
    "            x = self.transformerEncoder(x=x, mask=encoder_padding_mask)\n",
    "            x = self.transformerDecoder(x=x, y=y, mask=decoder_padding_mask)\n",
    "\n",
    "            return x\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Transformer model\".capitalize())\n",
    "    parser.add_argument(\n",
    "        \"--d_model\", type=int, default=512, help=\"Embedding dimension\".capitalize()\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--nhead\", type=int, default=8, help=\"Number of heads\".capitalize\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ffn\", type=int, default=2048, help=\"Feed forward dimension\".capitalize()\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--encoder_layers\",\n",
    "        type=int,\n",
    "        default=8,\n",
    "        help=\"Number of encoder layers\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--decoder_layers\",\n",
    "        type=int,\n",
    "        default=8,\n",
    "        help=\"Number of decoder layers\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dropout\", type=float, default=0.1, help=\"Dropout\".capitalize()\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eps\", type=float, default=1e-6, help=\"Layer norm epsilon\".capitalize()\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--display\", type=bool, default=False, help=\"Display model\".capitalize()\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    d_model = args.d_model\n",
    "    nhead = args.nhead\n",
    "    dim_feedforward = args.ffn\n",
    "    num_encoder_layers = args.encoder_layers\n",
    "    num_decoder_layers = args.decoder_layers\n",
    "    dropout = args.dropout\n",
    "    layer_norm_eps = args.eps\n",
    "\n",
    "    transformer = Transformer(\n",
    "        d_model=d_model,\n",
    "        nhead=nhead,\n",
    "        dim_feedforward=dim_feedforward,\n",
    "        num_encoder_layers=num_decoder_layers,\n",
    "        num_decoder_layers=num_decoder_layers,\n",
    "        dropout=dropout,\n",
    "        layer_norm_eps=layer_norm_eps,\n",
    "    )\n",
    "\n",
    "    X = torch.randn((40, 200, d_model))\n",
    "    y = torch.randn((40, 200, d_model))\n",
    "\n",
    "    encoder_padding_mask = torch.ones((40, 200))\n",
    "    decoder_padding_mask = torch.ones((40, 200))\n",
    "\n",
    "    assert (\n",
    "        transformer(\n",
    "            x=X,\n",
    "            y=y,\n",
    "            encoder_padding_mask=encoder_padding_mask,\n",
    "            decoder_padding_mask=decoder_padding_mask,\n",
    "        ).size()\n",
    "    ) == (\n",
    "        40,\n",
    "        200,\n",
    "        d_model,\n",
    "    ), \"Output of the transformer is not correct as the dimensions are not matching\".title()\n",
    "\n",
    "    if args.display:\n",
    "        print(\n",
    "            \"Total parameters of the transformer: \",\n",
    "            sum(params.numel() for params in transformer.parameters()),\n",
    "        )\n",
    "\n",
    "        path = config()[\"path\"][\"FILES_PATH\"]\n",
    "\n",
    "        draw_graph(\n",
    "            model=transformer,\n",
    "            input_data=(X, y, encoder_padding_mask, decoder_padding_mask),\n",
    "        ).visual_graph.render(filename=os.path.join(path, \"Transformer\"), format=\"png\")\n",
    "\n",
    "        print(f\"Decoder model saved in the path {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        text: list,\n",
    "        padding: str = \"max_length\",\n",
    "        truncation: bool = True,\n",
    "        return_tensors: str = \"pt\",\n",
    "        max_length: int = 200,\n",
    "        batch_size: int = 4,\n",
    "        return_attention_mask: bool = True,\n",
    "    ):\n",
    "        self.text = text\n",
    "        self.padding = padding\n",
    "        self.truncation = truncation\n",
    "        self.return_tensors = return_tensors\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "        self.return_attention_mask = return_attention_mask\n",
    "\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        except Exception as e:\n",
    "            print(\"An error occurred: \", e)\n",
    "\n",
    "    def tokenize_text(self):\n",
    "        if isinstance(self.text, list):\n",
    "            tokenizer_inputs = self.tokenizer(\n",
    "                self.text,\n",
    "                padding=self.padding,\n",
    "                truncation=self.truncation,\n",
    "                return_tensors=self.return_tensors,\n",
    "                max_length=self.max_length,\n",
    "                return_attention_mask=self.return_attention_mask,\n",
    "            )\n",
    "\n",
    "            return {\n",
    "                \"tokenize_inputs\": tokenizer_inputs,\n",
    "                \"input_ids\": tokenizer_inputs[\"input_ids\"],\n",
    "                \"attention_mask\": tokenizer_inputs[\"attention_mask\"],\n",
    "                \"vocab_size\": self.tokenizer.vocab_size,\n",
    "            }\n",
    "\n",
    "        else:\n",
    "            raise TypeError(\"Input must be a list of strings\".capitalize())\n",
    "\n",
    "    def create_dataloader(self):\n",
    "        try:\n",
    "            tokenize = self.tokenize_text()\n",
    "\n",
    "            input_ids = tokenize[\"input_ids\"]\n",
    "            attention_mask = tokenize[\"attention_mask\"]\n",
    "            vocab_size = tokenize[\"vocab_size\"]\n",
    "\n",
    "            datasets = TensorDataset(input_ids, attention_mask)\n",
    "            dataloader = DataLoader(\n",
    "                dataset=datasets, batch_size=self.batch_size, shuffle=True\n",
    "            )\n",
    "\n",
    "            dump(\n",
    "                value=dataloader,\n",
    "                filename=os.path.join(\n",
    "                    config()[\"path\"][\"PROCESSED_PATH\"], \"dataloader.pkl\"\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            return {\n",
    "                \"tokenizer_object\": self.tokenizer,\n",
    "                \"input_ids\": input_ids,\n",
    "                \"dataloader\": dataloader,\n",
    "                \"vocab_size\": vocab_size,\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"An error occurred: \", e)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tokenizer = Tokenizer(\n",
    "        text=english,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=200,\n",
    "        batch_size=4,\n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "\n",
    "    tokenizer = tokenizer.create_dataloader()\n",
    "\n",
    "    dataloader = tokenizer[\"dataloader\"]\n",
    "    vocab_size = tokenizer[\"vocab_size\"]\n",
    "\n",
    "    assert vocab_size == 30522"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script tests the implementation of a Transformer model from scratch using a dummy dataset. \n",
    "It includes tokenization, embedding, and the Transformer model itself. The script ensures that the \n",
    "code runs correctly and the Transformer model produces the expected output.\n",
    "\n",
    "Modules used:\n",
    "    - os\n",
    "    - sys\n",
    "    - transformers.AutoTokenizer\n",
    "    - utils: Provides the dummy English and German sentences.\n",
    "    - tokenizer: Custom Tokenizer class for tokenizing and processing text.\n",
    "    - embedding_layer: Custom EmbeddingLayer class for creating embeddings.\n",
    "    - transformer: Custom Transformer class for the Transformer model.\n",
    "\n",
    "The script performs the following steps:\n",
    "    1. Imports necessary modules and sets up paths.\n",
    "    2. Checks if the lengths of the English and German sentences are equal.\n",
    "    3. Initializes Tokenizers and DataLoaders for English and German sentences.\n",
    "    4. Initializes the Embedding Layer with the appropriate vocabulary size and dimensions.\n",
    "    5. Initializes the Transformer model with specified hyperparameters.\n",
    "    6. Tests the Transformer model by feeding it embeddings from the dummy dataset and prints the output size.\n",
    "\n",
    "Usage:\n",
    "    This script is intended to verify that the implemented Transformer model works correctly with a dummy dataset.\n",
    "    You can also use your own embeddings instead of the provided ones.\n",
    "\"\"\"\n",
    "\n",
    "# Variable values for easy configuration\n",
    "MAX_LENGTH = 200\n",
    "BATCH_SIZE = 40\n",
    "EMBEDDING_DIMENSION = 512\n",
    "NUM_ENCODER_LAYERS = 8\n",
    "NUM_DECODER_LAYERS = 8\n",
    "NUM_HEADS = 8\n",
    "DIM_FEEDFORWARD = 2048\n",
    "DROPOUT = 0.1\n",
    "LAYER_NORM_EPS = 1e-5\n",
    "\n",
    "# Ensure that the lengths of sentences match\n",
    "if len(english) != len(german):\n",
    "    raise ValueError(\"Length of the sentences are not equal\")\n",
    "\n",
    "# Initialize Tokenizers and DataLoaders\n",
    "english_tokenizer = Tokenizer(\n",
    "    text=english,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=MAX_LENGTH,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "english_tokenizer_results = english_tokenizer.create_dataloader()\n",
    "english_dataloader = english_tokenizer_results[\"dataloader\"]\n",
    "english_vocab_size = english_tokenizer_results[\"vocab_size\"]\n",
    "\n",
    "german_tokenizer = Tokenizer(\n",
    "    text=german,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=MAX_LENGTH,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "german_tokenizer_results = german_tokenizer.create_dataloader()\n",
    "german_dataloader = german_tokenizer_results[\"dataloader\"]\n",
    "german_vocab_size = german_tokenizer_results[\"vocab_size\"]\n",
    "\n",
    "# Initialize Embedding Layer\n",
    "embedding_layer = EmbeddingLayer(\n",
    "    vocabulary_size=english_vocab_size,\n",
    "    dimension=EMBEDDING_DIMENSION,\n",
    "    sequence_length=MAX_LENGTH,\n",
    ")\n",
    "\n",
    "# Initialize Transformer\n",
    "transformer_model = Transformer(\n",
    "    d_model=EMBEDDING_DIMENSION,\n",
    "    nhead=NUM_HEADS,\n",
    "    num_encoder_layers=NUM_ENCODER_LAYERS,\n",
    "    num_decoder_layers=NUM_DECODER_LAYERS,\n",
    "    dim_feedforward=DIM_FEEDFORWARD,\n",
    "    dropout=DROPOUT,\n",
    "    layer_norm_eps=LAYER_NORM_EPS,\n",
    ")\n",
    "\n",
    "# Test the Transformer with embeddings\n",
    "for (english_batch, english_padding_mask), (german_batch, german_padding_mask) in zip(\n",
    "    english_dataloader, german_dataloader\n",
    "):\n",
    "    english_embeddings = embedding_layer(english_batch)\n",
    "    german_embeddings = embedding_layer(german_batch)\n",
    "\n",
    "    transformer_output = transformer_model(\n",
    "        x=english_embeddings,\n",
    "        y=german_embeddings,\n",
    "        encoder_padding_mask=english_padding_mask,\n",
    "        decoder_padding_mask=german_padding_mask,\n",
    "    )\n",
    "    print(transformer_output.size())\n",
    "    break  # Test with only the first batch\n",
    "\n",
    "\n",
    "####################################################################################################################\n",
    "####################################################################################################################\n",
    "#                            THIS IS ANOTHER APPROACH THAT YOU CAN USE TO RUN THE TRANSFORMER                      #\n",
    "####################################################################################################################\n",
    "####################################################################################################################\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "############################\n",
    "#          English         #\n",
    "############################\n",
    "\n",
    "english_tokenizer = tokenizer(\n",
    "    english,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=MAX_LENGTH,\n",
    ")\n",
    "\n",
    "print(\"Tokenized Input IDs:\", english_tokenizer[\"input_ids\"].size())\n",
    "print(\"Attention Mask:\", english_tokenizer[\"attention_mask\"].size())\n",
    "\n",
    "print(\"*\" * 50, \"\\n\")\n",
    "\n",
    "english_vocab_size = tokenizer.vocab_size\n",
    "\n",
    "english_tokenizer_results = TensorDataset(\n",
    "    english_tokenizer[\"input_ids\"], english_tokenizer[\"attention_mask\"]\n",
    ")\n",
    "english_tokenizer_dataloader = DataLoader(\n",
    "    english_tokenizer_results, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "\n",
    "############################\n",
    "#          German          #\n",
    "############################\n",
    "\n",
    "german_tokenizer = tokenizer(\n",
    "    german,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=MAX_LENGTH,\n",
    ")\n",
    "\n",
    "print(\"Tokenized Input IDs:\", german_tokenizer[\"input_ids\"].size())\n",
    "print(\"Attention Mask:\", german_tokenizer[\"attention_mask\"].size())\n",
    "\n",
    "print(\"*\" * 50, \"\\n\")\n",
    "\n",
    "german_vocab_size = tokenizer.vocab_size\n",
    "\n",
    "german_tokenizer_results = TensorDataset(\n",
    "    german_tokenizer[\"input_ids\"], german_tokenizer[\"attention_mask\"]\n",
    ")\n",
    "german_tokenizer_dataloader = DataLoader(\n",
    "    german_tokenizer_results, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "\n",
    "###########################\n",
    "#         Embedding       #\n",
    "###########################\n",
    "\n",
    "assert german_vocab_size == english_vocab_size, \"Vocabulary sizes must be equal\"\n",
    "\n",
    "embedding = EmbeddingLayer(\n",
    "    vocabulary_size=english_vocab_size,\n",
    "    sequence_length=MAX_LENGTH,\n",
    "    dimension=EMBEDDING_DIMENSION,\n",
    ")\n",
    "\n",
    "# Test the Transformer with embeddings\n",
    "for (english_batch, english_padding_mask), (german_batch, german_padding_mask) in zip(\n",
    "    english_tokenizer_dataloader, german_tokenizer_dataloader\n",
    "):\n",
    "    english_embeddings = embedding(english_batch)\n",
    "    german_embeddings = embedding(german_batch)\n",
    "\n",
    "    transformer_output = transformer_model(\n",
    "        x=english_embeddings,\n",
    "        y=german_embeddings,\n",
    "        encoder_padding_mask=english_padding_mask,\n",
    "        decoder_padding_mask=german_padding_mask,\n",
    "    )\n",
    "    print(transformer_output.size())\n",
    "    break  # Test with only the first batch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPSG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
