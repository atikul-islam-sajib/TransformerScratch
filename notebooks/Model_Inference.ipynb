{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PMV2Q-v_Cv3"
      },
      "source": [
        "# Transformer from Scratch Tutorial\n",
        "\n",
        "This tutorial will guide you through the process of cloning the `TransformerScratch` repository from GitHub and running the code locally. This repository implements a Transformer model from scratch.\n",
        "\n",
        "## Step 1: Cloning the Repository\n",
        "\n",
        "First, let's clone the repository. Run the following command in your terminal:\n",
        "\n",
        "```bash\n",
        "!git clone https://github.com/atikul-islam-sajib/TransformerScratch.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0K4QVLP_GGt"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/atikul-islam-sajib/TransformerScratch.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qBpZTsn_WS3"
      },
      "source": [
        "## Step 2: Navigating to the Repository\n",
        "\n",
        "Once the repository is cloned, navigate to the repository directory. If you are using a Jupyter notebook, you can change the directory with the following command:\n",
        "\n",
        "```python\n",
        "%cd /content/TransformerScratch\n",
        "```\n",
        "\n",
        "If you are using a terminal, use:\n",
        "\n",
        "```bash\n",
        "cd TransformerScratch\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8r8VYfc8CO1"
      },
      "outputs": [],
      "source": [
        "%cd /content/TransformerScratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8s7Knic5_eVN"
      },
      "source": [
        "## Step 4: Installing Dependencies\n",
        "\n",
        "Before running the code, you need to install the required dependencies. These are usually listed in a `requirements.txt` file or mentioned in the `README.md`. Install them using `pip`:\n",
        "\n",
        "```python\n",
        "!pip install -r requirements.txt\n",
        "```\n",
        "\n",
        "If there's no `requirements.txt` file, you may need to manually install the required packages mentioned in the `README.md` or the source code files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzMhx4BY8Jg6"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gHbPZJhAHWx"
      },
      "source": [
        "## Step 4: Setting Up the Data\n",
        "\n",
        "Prepare the data for the model. Here, we use sample sentences in English and German:\n",
        "\n",
        "```python\n",
        "english = [\n",
        "    \"The sun is shining brightly today\",\n",
        "    \"I enjoy reading books on rainy afternoons\",\n",
        "    \"The cat sat on the windowsill watching the birds\",\n",
        "    \"She baked a delicious chocolate cake for dessert\",\n",
        "    \"We went for a long walk in the park yesterday\",\n",
        "]\n",
        "\n",
        "german = [\n",
        "    \"Die Sonne scheint heute hell\",\n",
        "    \"Ich lese gerne Bücher an regnerischen Nachmittagen\",\n",
        "    \"Die Katze saß auf der Fensterbank und beobachtete die Vögel\",\n",
        "    \"Sie hat einen leckeren Schokoladenkuchen zum Nachtisch gebacken\",\n",
        "    \"Wir sind gestern lange im Park spazieren gegangen\",\n",
        "]\n",
        "\n",
        "if len(english) != len(german):\n",
        "    raise ValueError(\"Length of the sentences are not equal\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2K78qiD6AKBv"
      },
      "outputs": [],
      "source": [
        "english = [\n",
        "    \"The sun is shining brightly today\",\n",
        "    \"I enjoy reading books on rainy afternoons\",\n",
        "    \"The cat sat on the windowsill watching the birds\",\n",
        "    \"She baked a delicious chocolate cake for dessert\",\n",
        "    \"We went for a long walk in the park yesterday\",\n",
        "]\n",
        "\n",
        "german = [\n",
        "    \"Die Sonne scheint heute hell\",\n",
        "    \"Ich lese gerne Bücher an regnerischen Nachmittagen\",\n",
        "    \"Die Katze saß auf der Fensterbank und beobachtete die Vögel\",\n",
        "    \"Sie hat einen leckeren Schokoladenkuchen zum Nachtisch gebacken\",\n",
        "    \"Wir sind gestern lange im Park spazieren gegangen\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpSJaChHAOYh"
      },
      "source": [
        "## Step 5: Defining Parameters\n",
        "\n",
        "Define the parameters for the Transformer model:\n",
        "\n",
        "```python\n",
        "MAX_LENGTH = 200           # Maximum length of the input sequences\n",
        "BATCH_SIZE = 2             # Number of samples per batch\n",
        "EMBEDDING_DIMENSION = 512  # Dimensionality of the embedding vectors\n",
        "NUM_ENCODER_LAYERS = 8     # Number of encoder layers in the Transformer\n",
        "NUM_DECODER_LAYERS = 8     # Number of decoder layers in the Transformer\n",
        "NUM_HEADS = 8              # Number of attention heads\n",
        "DIM_FEEDFORWARD = 2048     # Dimensionality of the feedforward network\n",
        "DROPOUT = 0.1              # Dropout rate\n",
        "LAYER_NORM_EPS = 1e-5      # Epsilon for layer normalization\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIe7k2HlAR-M"
      },
      "outputs": [],
      "source": [
        "# Variable values for easy configuration\n",
        "MAX_LENGTH = 200\n",
        "BATCH_SIZE = 2\n",
        "EMBEDDING_DIMENSION = 512\n",
        "NUM_ENCODER_LAYERS = 8\n",
        "NUM_DECODER_LAYERS = 8\n",
        "NUM_HEADS = 8\n",
        "DIM_FEEDFORWARD = 2048\n",
        "DROPOUT = 0.1\n",
        "LAYER_NORM_EPS = 1e-5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEA3WMawAhl9"
      },
      "source": [
        "## Step 6: Initializing Tokenizers\n",
        "\n",
        "Initialize the tokenizers for both English and German sentences:\n",
        "\n",
        "```python\n",
        "from src.tokenizer import Tokenizer\n",
        "\n",
        "english_tokenizer = Tokenizer(\n",
        "    text=english,\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\",\n",
        "    max_length=MAX_LENGTH,\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "english_tokenizer_results = english_tokenizer.create_dataloader()\n",
        "english_dataloader = english_tokenizer_results[\"dataloader\"]\n",
        "english_vocab_size = english_tokenizer_results[\"vocab_size\"]\n",
        "\n",
        "german_tokenizer = Tokenizer(\n",
        "    text=german,\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\",\n",
        "    max_length=MAX_LENGTH,\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "german_tokenizer_results = german_tokenizer.create_dataloader()\n",
        "german_dataloader = german_tokenizer_results[\"dataloader\"]\n",
        "german_vocab_size = german_tokenizer_results[\"vocab_size\"]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdXjtBthAknc"
      },
      "outputs": [],
      "source": [
        "from src.tokenizer import Tokenizer\n",
        "\n",
        "english_tokenizer = Tokenizer(\n",
        "    text=english,\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\",\n",
        "    max_length=MAX_LENGTH,\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "\n",
        "\n",
        "german_tokenizer = Tokenizer(\n",
        "    text=german,\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\",\n",
        "    max_length=MAX_LENGTH,\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "\n",
        "english_tokenizer_results = english_tokenizer.create_dataloader()\n",
        "english_dataloader = english_tokenizer_results[\"dataloader\"]\n",
        "english_vocab_size = english_tokenizer_results[\"vocab_size\"]\n",
        "\n",
        "german_tokenizer_results = german_tokenizer.create_dataloader()\n",
        "german_dataloader = german_tokenizer_results[\"dataloader\"]\n",
        "german_vocab_size = german_tokenizer_results[\"vocab_size\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLmtW8l8Aqs-"
      },
      "source": [
        "## Step 7: Initializing the Embedding Layer\n",
        "\n",
        "Create the embedding layer:\n",
        "\n",
        "```python\n",
        "from src.embedding_layer import EmbeddingLayer\n",
        "\n",
        "embedding_layer = EmbeddingLayer(\n",
        "    vocabulary_size=english_vocab_size,\n",
        "    dimension=EMBEDDING_DIMENSION,\n",
        "    sequence_length=MAX_LENGTH,\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XCY2WA_As8U"
      },
      "outputs": [],
      "source": [
        "from src.embedding_layer import EmbeddingLayer\n",
        "\n",
        "embedding_layer = EmbeddingLayer(\n",
        "    vocabulary_size=english_vocab_size,\n",
        "    dimension=EMBEDDING_DIMENSION,\n",
        "    sequence_length=MAX_LENGTH,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71QVhnrdAzJ8"
      },
      "source": [
        "\n",
        "## Step 8: Initializing the Transformer Model\n",
        "\n",
        "Set up the Transformer model:\n",
        "\n",
        "```python\n",
        "from src.transformer import Transformer\n",
        "\n",
        "transformer_model = Transformer(\n",
        "    d_model=EMBEDDING_DIMENSION,\n",
        "    nhead=NUM_HEADS,\n",
        "    num_encoder_layers=NUM_ENCODER_LAYERS,\n",
        "    num_decoder_layers=NUM_DECODER_LAYERS,\n",
        "    dim_feedforward=DIM_FEEDFORWARD,\n",
        "    dropout=DROPOUT,\n",
        "    layer_norm_eps=LAYER_NORM_EPS,\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVNonJrRA2Xi"
      },
      "outputs": [],
      "source": [
        "from src.transformer import Transformer\n",
        "\n",
        "transformer_model = Transformer(\n",
        "    d_model=EMBEDDING_DIMENSION,\n",
        "    nhead=NUM_HEADS,\n",
        "    num_encoder_layers=NUM_ENCODER_LAYERS,\n",
        "    num_decoder_layers=NUM_DECODER_LAYERS,\n",
        "    dim_feedforward=DIM_FEEDFORWARD,\n",
        "    dropout=DROPOUT,\n",
        "    layer_norm_eps=LAYER_NORM_EPS,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LMvpZ9QA84V"
      },
      "source": [
        "## Step 9: Testing the Transformer Model\n",
        "\n",
        "Test the Transformer model with the embeddings from the first batch:\n",
        "\n",
        "```python\n",
        "for (english_batch, english_padding_mask), (german_batch, german_padding_mask) in zip(\n",
        "    english_dataloader, german_dataloader\n",
        "):\n",
        "    english_embeddings = embedding_layer(english_batch)\n",
        "    german_embeddings = embedding_layer(german_batch)\n",
        "\n",
        "    transformer_output = transformer_model(\n",
        "        x=english_embeddings,\n",
        "        y=german_embeddings,\n",
        "        encoder_padding_mask=english_padding_mask,\n",
        "        decoder_padding_mask=german_padding_mask,\n",
        "    )\n",
        "    print(transformer_output.size())\n",
        "    break  # Test with only the first batch\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYkkDUg49GtM"
      },
      "outputs": [],
      "source": [
        "# Test the Transformer with embeddings\n",
        "for (english_batch, english_padding_mask), (german_batch, german_padding_mask) in zip(\n",
        "    english_dataloader, german_dataloader\n",
        "):\n",
        "    english_embeddings = embedding_layer(english_batch)\n",
        "    german_embeddings = embedding_layer(german_batch)\n",
        "\n",
        "    transformer_output = transformer_model(\n",
        "        x=english_embeddings,\n",
        "        y=german_embeddings,\n",
        "        encoder_padding_mask=english_padding_mask,\n",
        "        decoder_padding_mask=german_padding_mask,\n",
        "    )\n",
        "    print(transformer_output.size())\n",
        "    break  # Test with only the first batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Is55mKLRA1J7"
      },
      "outputs": [],
      "source": [
        "from src.transformer import Transformer\n",
        "\n",
        "transformer_model = Transformer(\n",
        "    d_model=EMBEDDING_DIMENSION,\n",
        "    nhead=NUM_HEADS,\n",
        "    num_encoder_layers=NUM_ENCODER_LAYERS,\n",
        "    num_decoder_layers=NUM_DECODER_LAYERS,\n",
        "    dim_feedforward=DIM_FEEDFORWARD,\n",
        "    dropout=DROPOUT,\n",
        "    layer_norm_eps=LAYER_NORM_EPS,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGTgqOSe-rzD"
      },
      "source": [
        "# THIS IS ANOTHER APPROACH THAT YOU CAN USE TO RUN THE TRANSFORMER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpgIPk7q9Z4P"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "\n",
        "############################\n",
        "#          English         #\n",
        "############################\n",
        "\n",
        "english_tokenizer = tokenizer(\n",
        "    english,\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\",\n",
        "    max_length=MAX_LENGTH,\n",
        ")\n",
        "\n",
        "print(\"Tokenized Input IDs:\", english_tokenizer[\"input_ids\"].size())\n",
        "print(\"Attention Mask:\", english_tokenizer[\"attention_mask\"].size())\n",
        "\n",
        "print(\"*\" * 50, \"\\n\")\n",
        "\n",
        "english_vocab_size = tokenizer.vocab_size\n",
        "\n",
        "english_tokenizer_results = TensorDataset(\n",
        "    english_tokenizer[\"input_ids\"], english_tokenizer[\"attention_mask\"]\n",
        ")\n",
        "english_tokenizer_dataloader = DataLoader(\n",
        "    english_tokenizer_results, batch_size=BATCH_SIZE, shuffle=True\n",
        ")\n",
        "\n",
        "############################\n",
        "#          German          #\n",
        "############################\n",
        "\n",
        "german_tokenizer = tokenizer(\n",
        "    german,\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\",\n",
        "    max_length=MAX_LENGTH,\n",
        ")\n",
        "\n",
        "print(\"Tokenized Input IDs:\", german_tokenizer[\"input_ids\"].size())\n",
        "print(\"Attention Mask:\", german_tokenizer[\"attention_mask\"].size())\n",
        "\n",
        "print(\"*\" * 50, \"\\n\")\n",
        "\n",
        "german_vocab_size = tokenizer.vocab_size\n",
        "\n",
        "german_tokenizer_results = TensorDataset(\n",
        "    german_tokenizer[\"input_ids\"], german_tokenizer[\"attention_mask\"]\n",
        ")\n",
        "german_tokenizer_dataloader = DataLoader(\n",
        "    german_tokenizer_results, batch_size=BATCH_SIZE, shuffle=True\n",
        ")\n",
        "\n",
        "###########################\n",
        "#         Embedding       #\n",
        "###########################\n",
        "\n",
        "assert german_vocab_size == english_vocab_size, \"Vocabulary sizes must be equal\"\n",
        "\n",
        "embedding = EmbeddingLayer(\n",
        "    vocabulary_size=english_vocab_size,\n",
        "    sequence_length=MAX_LENGTH,\n",
        "    dimension=EMBEDDING_DIMENSION,\n",
        ")\n",
        "\n",
        "# Test the Transformer with embeddings\n",
        "for (english_batch, english_padding_mask), (german_batch, german_padding_mask) in zip(\n",
        "    english_tokenizer_dataloader, german_tokenizer_dataloader\n",
        "):\n",
        "    english_embeddings = embedding(english_batch)\n",
        "    german_embeddings = embedding(german_batch)\n",
        "\n",
        "    transformer_output = transformer_model(\n",
        "        x=english_embeddings,\n",
        "        y=german_embeddings,\n",
        "        encoder_padding_mask=english_padding_mask,\n",
        "        decoder_padding_mask=german_padding_mask,\n",
        "    )\n",
        "    print(transformer_output.size())\n",
        "    break  # Test with only the first batch\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
